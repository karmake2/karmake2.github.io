<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-03-18T12:47:56-07:00</updated><id>/feed.xml</id><title type="html">Academic Homepage</title><subtitle>personal description</subtitle><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><entry><title type="html">Project TextMall: Text-Mining Made Simple for All</title><link href="/posts/2020/01/TextMall/" rel="alternate" type="text/html" title="Project TextMall: Text-Mining Made Simple for All" /><published>2020-11-01T00:00:00-07:00</published><updated>2020-11-01T00:00:00-07:00</updated><id>/posts/2020/01/TextMall</id><content type="html" xml:base="/posts/2020/01/TextMall/">&lt;p&gt;The objective of this research is to develop a general-purpose text analytics platform, i.e., Text-Mall, which would enable real-world users to easily explore the power of Text-Mining in a simple and interactive fashion without worrying about the underlying details of Natural Language Processing.&lt;/p&gt;

&lt;center&gt;
  &lt;img src=&quot;/images/students/TextMall.png&quot; alt=&quot;Team TextMall&quot; width=&quot;500&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-textmall&quot;&gt;Overview of Project TextMall&lt;/h1&gt;
&lt;p&gt;Despite great progress in the field of natural language processing (NLP), computers are still far from being able to accurately understand unrestricted natural language. Indeed, how to develop a completely automated text analytics system that can support many different &lt;em&gt;Text-Mining&lt;/em&gt; applications is a very hard problem;  as such, &lt;em&gt;Text-Mining&lt;/em&gt; technology is not still directly usable by real world users like doctors, teachers,  executives etc. Thus, it is essential to involve humans (real users) in the loop in an &lt;em&gt;interactive&lt;/em&gt; fashion and facilitate explorative Text-Mining to enable end-users to apply this technology effectively in the real world.&lt;/p&gt;

&lt;p&gt;The objective of this research is to develop such a general purpose text analytics platform which would enable real-world users to easily explore the power of Text-Mining in a simple and interactive fashion without worrying about the underlying details of NLP. The PI aims to address this challenge by developing a new general-purpose text analytics platform, i.e., Text-Mall (aka. Text Mining for all), which will provide simple intuitive operators for interactive text-mining tasks which can be easily explained and taught to the general public.&lt;/p&gt;

&lt;p&gt;Here is an example use-case of TextMall agent:&lt;/p&gt;

&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/images/TextMall.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:450px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This is the high level architecture of TextMall agent:&lt;/p&gt;
&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/files/Publications/2018/SOFSAT.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:300px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;impact-of-project-textmall&quot;&gt;Impact of Project TextMall&lt;/h1&gt;
&lt;p&gt;Text-Mining is essential for optimizing all kinds of decisions related
to people, ranging from making effective and acceptable public policies
by governments, to deciding critical medical treatment for patients by
doctors, to providing personalized tutoring materials to students by
teachers, and/or to effective advertising of products to people on the
Internet by companies. &lt;em&gt;Text-Mall&lt;/em&gt;, once realized, will enable real
world users to conduct such analysis and thus, facilitate more informed
decision making process for them. &lt;em&gt;Text-Mall&lt;/em&gt; operators will also be
combined flexibly to support a wide range of analysis tasks that may
require different workflows, thus enabling an application developer to
&lt;em&gt;program&lt;/em&gt; a text mining application by using &lt;em&gt;Text-Mall&lt;/em&gt; as a
programming language for text analysis.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">The objective of this research is to develop a general-purpose text analytics platform, i.e., Text-Mall, which would enable real-world users to easily explore the power of Text-Mining in a simple and interactive fashion without worrying about the underlying details of Natural Language Processing.</summary></entry><entry><title type="html">Project VIDS: Virtual Interactive Data Scientist</title><link href="/posts/2020/02/VIDS/" rel="alternate" type="text/html" title="Project VIDS: Virtual Interactive Data Scientist" /><published>2020-10-02T00:00:00-07:00</published><updated>2020-10-02T00:00:00-07:00</updated><id>/posts/2020/02/VIDS</id><content type="html" xml:base="/posts/2020/02/VIDS/">&lt;p&gt;What if an AI agent can serve you as your personal data scientist? Isn’t that cool?&lt;/p&gt;

&lt;center&gt;
  &lt;img src=&quot;/images/students/VIDS.png&quot; alt=&quot;Team TextMall&quot; width=&quot;500&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-vids-a-conversational-ai-agent-to-help-with-data-science-tasks&quot;&gt;Overview of Project VIDS (a conversational AI agent to help with Data Science Tasks)&lt;/h1&gt;
&lt;p&gt;Automated machine learning (AutoML) is the process of automating the application of machine learning to real-world problems. The primary goals of AutoML tools are to provide methods and processes to make Machine Learning available for non-Machine Learning experts (domain experts), to improve efficiency of Machine Learning, and to accelerate research on Machine Learning. But although automation and efficiency are some of AutoML’s main selling points, the process still requires a surprising level of human involvement. A number of vital steps of the machine learning pipeline, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training data set etc., still tend to be done manually by a data scientist or a machine learning engineer on an ad-hoc basis.&lt;/p&gt;

&lt;p&gt;How can we build an end-to-end automated pipeline of machine learning and enable domain experts to directly engage with the fascinating world of big data, and thus make the machine learning pipeline more efficient and appealing? We propose to address this question by laying out his vision for a Virtual Interactive Data Scientist (VIDS). VIDS can be thought of as an intelligent agent which, given a large corpus of data, starts talking to the domain experts like a human data scientist in order to quickly figure out the user’s need and explore alternative prediction opportunities. It will automatically formulate different prediction tasks, as well as explore alternative learning models to recommend the best one to the users.&lt;/p&gt;

&lt;p&gt;Here is an example use-case of VIDS agent:&lt;/p&gt;

&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/images/VIDS.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:500px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This is the high level architecture of VIDS agent:&lt;/p&gt;
&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/images/VIDSCycleVertical.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:500px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;impact-of-project-vids&quot;&gt;Impact of Project VIDS&lt;/h1&gt;
&lt;p&gt;VIDS will communicate with users like a human data scientist and quickly understand their needs. It will translate these needs into well-defined prediction tasks. VIDS will enable the domain experts to directly engage with the fascinating world of big data without worrying about the underlying detailed knowledge of machine learning. VIDS will also automate many of the routine tasks performed by a &lt;em&gt;human&lt;/em&gt; Data Scientist. Although this is a highly ambitious goal, if reached, it will expedite the implementation of machine learning systems allowing ML to appeal to a far broader audience across various domains. In Summary, VIDS will enable end-users (machine learning non-experts) to interactively play with infinite combinations of different prediction tasks and AutoML models by tweaking a minimal number of hyper-parameters and task preferences.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">What if an AI agent can serve you as your personal data scientist? Isn’t that cool?</summary></entry><entry><title type="html">Project Annotate: Zero-Shot Topic Inference</title><link href="/posts/2020/01/Annotate/" rel="alternate" type="text/html" title="Project Annotate: Zero-Shot Topic Inference" /><published>2020-10-01T00:00:00-07:00</published><updated>2020-10-01T00:00:00-07:00</updated><id>/posts/2020/01/Annotate</id><content type="html" xml:base="/posts/2020/01/Annotate/">&lt;p&gt;Information retrieval and Knowledge mining become much easier if data is categorized and annotated precisely. With the rapid growth of Big-data, it is infeasible to perform manual annotation, as it is slow and expensive. Although the area of text annotation is not in the nascent phase, it has not been well-studied from a user-centric point of view, which is the goal of this project.&lt;/p&gt;

&lt;center&gt;
  &lt;img src=&quot;/images/students/Annotate.png&quot; alt=&quot;Team TextMall&quot; width=&quot;500&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-annotate&quot;&gt;Overview of Project Annotate&lt;/h1&gt;
&lt;p&gt;Text data is highly unstructured and can often be viewed as a complex representation of different concepts, entities, events, sentiments etc. For a wide variety of computational tasks, it is thus very important to annotate text data with the associated concepts / entities, which can put some initial structure / index on raw text data. However, It is not feasible to manually annotate a large amount of text, raising the need for automatic text annotation.&lt;/p&gt;

&lt;p&gt;In this project, we focus on concept annotation in text data from the perspective of real world users. Concept annotation is not a trivial task and its utility often highly relies on the preference of the user. Despite significant progress in natural language processing research, we still lack a general purpose concept annotation tool which can effectively serve users from a wide range of application domains. Thus, further investigation is needed from a user-centric point of view to design an automated concept annotation tool that will ensure maximum utility to its users. To achieve this goal, we created a benchmark corpus of two real world data-sets, i.e., “News Concept Data-set” and  “Medical Concept Data-set”, to introduce the notion of user-oriented concept annotation and provide a way to evaluate this task. The term “user-centric” means that the desired concepts are defined as well as characterized by the users themselves. We proposed a new approach based on generative feature language models that can mine the implicit concepts more effectively through unsupervised statistical learning. The parameters are optimized automatically using an Expectation-Maximization algorithm.&lt;/p&gt;

&lt;p&gt;This is the high level architecture of Concept Annotation:&lt;/p&gt;
&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/files/Publications/2016/ImplicitFeature.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:300px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;impact-of-project-annotate&quot;&gt;Impact of Project Annotate&lt;/h1&gt;
&lt;p&gt;Concept Annotation can be viewed as adding topic-related metadata to a text article. The idea of concept annotation is not new and several researchers have studied this problem from different perspectives in the past. Still, concept annotation is not a trivial task and its utility often relies highly on the preference of the user. Indeed, the ultimate goal of any intelligent tool is to serve the need of the end users and thus, its design principles should primarily focus on the real-world application scenarios involving the end users. Despite significant progress in natural language processing research, we still lack a general purpose concept annotation tool which can effectively serve users from a wide range of application domains. The main reason behind this limitation is the absence of a user-centric study of the concept annotation task encompassing a more realistic scenario. Thus, further investigation is needed from a user-centric point of view to design an automated concept annotation tool that will ensure maximum utility to its users.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">Information retrieval and Knowledge mining become much easier if data is categorized and annotated precisely. With the rapid growth of Big-data, it is infeasible to perform manual annotation, as it is slow and expensive. Although the area of text annotation is not in the nascent phase, it has not been well-studied from a user-centric point of view, which is the goal of this project.</summary></entry><entry><title type="html">Project A2I-MOOC: Artificially Intelligent and Interactive MOOCs</title><link href="/posts/2020/03/A2I-MOOC/" rel="alternate" type="text/html" title="Project A2I-MOOC: Artificially Intelligent and Interactive MOOCs" /><published>2020-03-01T00:00:00-08:00</published><updated>2020-03-01T00:00:00-08:00</updated><id>/posts/2020/03/A2I-MOOC</id><content type="html" xml:base="/posts/2020/03/A2I-MOOC/">&lt;p&gt;MOOCs have abysmal retention rates (5-15%) and high student failure rates (7-13%). In this project, we propose two ways to increase student engagement in MOOCs and other online courses through an artificially intelligent system that leverages machine learning and natural language processing. This system will (1) process, prioritize and organize students’ questions in real-time and provide the most relevant questions to instructors for answering during their live lectures, and (2) automate the creation of breakout rooms (which have recently become popular in Zoom classes) based on high-interest topics emerging from student questions and populated by like-minded students during live lectures.&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-a2i-mooc&quot;&gt;Overview of Project A2I-MOOC&lt;/h1&gt;
&lt;p&gt;Today, online education is widely accepted, highly regarded, and completely necessary for educating a diverse and geographically distributed student population. Massive, Open, Online Courses (MOOCs), first introduced in 2008, have emerged as a popular approach to delivering distance education since 2012. The massive move to online education by educational institutions in early 2020 due to the COVID pandemic has further transformed teaching and learning for students and instructors. It is quite likely that these high levels of online education will continue even after normalcy is restored in society. Online courses, especially when there is a large teacher-to-student ratio, lead to significantly less opportunities for students to ask questions and have them answered by the teacher in real-time, and to discuss course topics among themselves during lectures. The flow of knowledge is often unidirectional, and students lack a supportive environment where they can learn from their peers. Instructors can quickly feel overwhelmed with many questions from large numbers of students, and due to the remote nature and geographically dispersed student body, students have very limited interactions with each other during live lectures. This can lead to student disengagement, and negatively affect learning. Not surprisingly, MOOCs have abysmal retention rates (5-15%) and high student failure rates (7-13%).
Therefore, we propose two ways to increase student engagement in MOOCs and other online courses through an artificially intelligent system that leverages machine learning and natural language processing. This system will (1) process, prioritize and organize students’ questions in real-time and provide the most relevant questions to instructors for answering during their live lectures, and (2) automate the creation of breakout rooms (which have recently become popular in Zoom classes) based on high-interest topics emerging from student questions and populated by like-minded students during live lectures. A two-stage iterative design will be employed to ensure that a usable and effective system will be developed by the end of the project.&lt;/p&gt;

&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/images/A2I-MOOC.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:450px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;impact-of-project-a2i-mooc&quot;&gt;Impact of Project A2I-MOOC&lt;/h1&gt;
&lt;p&gt;Online and remote learning, in the form of MOOCs or as part of K-16 education in general, is now a reality given the situation with COVID. This proposal addresses a key problem in this environment: the lack of opportunity for students to engage in the class and with their peers by asking questions (and having them answered) and participating in small-group discussions. Results from this project have the potential of impacting a large number of students who remotely take courses that still rely heavily on the traditional lecture format. The tool that we develop will work for courses on any subject matter, and will be made freely available to educational institutions across the country for use in any online course or MOOC, thereby potentially benefiting thousands of students. The project will enhance educational technology research in the EPSCoR state of Alabama, and provide graduate and undergraduate students at Auburn University in Alabama to conduct innovative research on artificial intelligence in education.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">MOOCs have abysmal retention rates (5-15%) and high student failure rates (7-13%). In this project, we propose two ways to increase student engagement in MOOCs and other online courses through an artificially intelligent system that leverages machine learning and natural language processing. This system will (1) process, prioritize and organize students’ questions in real-time and provide the most relevant questions to instructors for answering during their live lectures, and (2) automate the creation of breakout rooms (which have recently become popular in Zoom classes) based on high-interest topics emerging from student questions and populated by like-minded students during live lectures.</summary></entry><entry><title type="html">Project Robust IR and NLP Evaluation</title><link href="/posts/2020/02/Evaluate/" rel="alternate" type="text/html" title="Project Robust IR and NLP Evaluation" /><published>2020-02-01T00:00:00-08:00</published><updated>2020-02-01T00:00:00-08:00</updated><id>/posts/2020/02/Evaluate</id><content type="html" xml:base="/posts/2020/02/Evaluate/">&lt;p&gt;We propose the new framework of IR evaluation with both upper and lower bound (UL) normalization of traditional metrics and systematically study the effect of UL-normalization on three popular evaluation metrics. We also propose three different variations of the proposed upper and lower bound (UL) normalized evaluation framework and experiment each of them with three evaluation metrics individually, creating nine new evaluation metrics in total. We show how we can compute more realistic query-specific lower-bounds for evaluation metrics by computing their expected values for each query in case of a randomized ranking of the corresponding documents. We also theoretically prove their correctness.&lt;/p&gt;

&lt;center&gt;
  &lt;img src=&quot;/images/students/Evaluation.png&quot; alt=&quot;Team Evaluation&quot; width=&quot;500&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-robust-ir-and-nlp-evaluation&quot;&gt;Overview of Project Robust IR and NLP Evaluation&lt;/h1&gt;
&lt;p&gt;Previous studies have shown that popular Information Retrieval (IR) evaluation metrics, e.g., $nDCG$, $ERR$, $MAP$, are not robust with respect to the variation of the cutoff rank $k$. In this paper, our main goal is to investigate how we can make IR evaluation metrics more robust; more specifically, less sensitive to the variation of cutoff $k$. We start our investigation with the observation that both $nDCG$ and $MAP$ metrics only involve query-specific upper-bound normalization (e.g., normalization with Ideal DCG for $nDCG$ computation); however, none of them include a query-specific lower-bound normalization. For $ERR$, it has neither upper nor lower-bound normalization. We then hypothesize that the high sensitivity of conventional IR evaluation metrics w.r.t. cutoff $k$ is partly due to the fact that none of them include a query-specific lower-bound normalization. To test this hypothesis, we proposed a new general framework for IR evaluation with both upper and lower bound normalization, instantiated the new framework for three popular IR evaluation metrics: $nDCG$, $ERR$, $MAP$ and compared against the corresponding traditional metrics without the proposed normalization. Experimental results show that the proposed upper and lower bound normalized version is indeed more robust (less sensitive) w.r.t. the variation of $k$ for all three metrics studied, decreasing the standard deviation of the original metrics substantially (on average around {68%}) and thus, proving our hypothesis. Hence, we urge the community to use both upper and lower bound normalization while evaluating any IR system from now onward.&lt;/p&gt;

&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/images/nDCG-robust.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:450px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;impact-of-project-robust-ir-and-nlp-evaluation&quot;&gt;Impact of Project Robust IR and NLP Evaluation&lt;/h1&gt;
&lt;p&gt;Empirical evaluation is a key challenge for any information retrieval (IR) system. The success of an IR system largely depends on the user’s satisfaction, thus an accurate evaluation metric is very important for measuring the perceived utility of a retrieval system by the real users. However, previous studies have shown that popular IR evaluation metrics, like $nDCG$, $ERR$, $MAP$, are not robust in terms of accurately capturing the utility perceived by an individual user. For example, in most existing works, $nDCG$ is computed for a user-defined cutoff $k$, i.e., $nDCG@k$, across all queries by taking an average over them. Unfortunately, this practice is problematic for two reasons: 1) Every query is different and the same cut-off $k$ does not accurately capture a user’s browsing behavior as well as their perceived utility for all different queries, and 2) Given a particular query, the relative performances of different ranking methods (in terms of $nDCG@k$) vary heavily as we vary $k$ and at the same time, result in high standard deviation (SD) in $nDCG@k$ scores for varying $k$. This clearly suggests the current practice of computing $nDCG@k$ is not robust. In this project, we are working towards addressing these limitations.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">We propose the new framework of IR evaluation with both upper and lower bound (UL) normalization of traditional metrics and systematically study the effect of UL-normalization on three popular evaluation metrics. We also propose three different variations of the proposed upper and lower bound (UL) normalized evaluation framework and experiment each of them with three evaluation metrics individually, creating nine new evaluation metrics in total. We show how we can compute more realistic query-specific lower-bounds for evaluation metrics by computing their expected values for each query in case of a randomized ranking of the corresponding documents. We also theoretically prove their correctness.</summary></entry><entry><title type="html">Project Deep-MD: Molecular Dynamics Modeling with Deep Time series Forecasting</title><link href="/posts/2020/01/Deep-MD/" rel="alternate" type="text/html" title="Project Deep-MD: Molecular Dynamics Modeling with Deep Time series Forecasting" /><published>2020-01-01T00:00:00-08:00</published><updated>2020-01-01T00:00:00-08:00</updated><id>/posts/2020/01/MD</id><content type="html" xml:base="/posts/2020/01/Deep-MD/">&lt;p&gt;We explore machine-learning methodologies for predicting the outcomes of MD simulations by preserving their accurate time labels. This idea will greatly reduce the computational expenses associated with performing MD, making it broadly accessible beyond the current user-base of scientific researchers to high schools and colleges, where the computational resources are sparse.&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-deep-md&quot;&gt;Overview of Project Deep-MD&lt;/h1&gt;
&lt;p&gt;A major thrust of Biophyiscs investigations is to determine the function of proteins in living cells. Addressing this seminal question, three-decades of physics- and data-based computations as well as experiments have been dedicated to resolve protein structures from their sequence, and the area continues to grow. However, dynamics of the proteins (and not just their stationary structures frozen in time) are key for biological functions. Computational modeling of any complex dynamic system essentially boils down to a multivariate time series forecasting task, and hence a time series trajectory data of an evolving biological system is essential to analyze and computationally learn the underlying molecular physics. The specialities that biomolecular dynamics presents in our submitted data sets is that, the underlying data-structure of molecular geometries is high-dimensional (can grow up to 100 million Cartesian dimensions), and the associated time series switches between fast and slowly evolving regimes. Learning and predicting from this high-dimensional rapidly changing data will open new paradigms in the area of molecular simulation of protein dynamics, where our software, NAMD serves a community of nearly 20,000 users. Beyond serving as case studies for the application of LSTM and linear regression methods, applications of resolving these sets will allow real-world applications in the interpretation of data from Atomic Force Microscopy experiments.&lt;/p&gt;

&lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;https://karmake2.github.io/images/Deep-MD.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:450px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;impact-of-project-deep-md&quot;&gt;Impact of Project Deep-MD&lt;/h1&gt;
&lt;p&gt;Molecular dynamics or MD simulations have emerged to become the cornerstone of today’s computational biophysics, enabling the description of structure-function relationships at atomistic details. These simulations have brought forth milestone discoveries including resolving the mechanisms of drug-protein interactions, protein synthesis and membrane transport, molecular motors and biological energy transfer, and viral maturation, encompassing a number of our contributions. More recently, we have employed molecular modeling to predict mortality rates from SARS-Cov-2 showcasing its application in epidemiology.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">We explore machine-learning methodologies for predicting the outcomes of MD simulations by preserving their accurate time labels. This idea will greatly reduce the computational expenses associated with performing MD, making it broadly accessible beyond the current user-base of scientific researchers to high schools and colleges, where the computational resources are sparse.</summary></entry><entry><title type="html">Research Statement 2018. (Archived)</title><link href="/posts/2018/12/blog-post-1/" rel="alternate" type="text/html" title="Research Statement 2018. (Archived)" /><published>2018-12-01T00:00:00-08:00</published><updated>2018-12-01T00:00:00-08:00</updated><id>/posts/2018/12/blog-post-1</id><content type="html" xml:base="/posts/2018/12/blog-post-1/">&lt;p&gt;Broadly, my research interest lies at the intersection of Text Mining, Natural Language Processing and Information Retrieval. More specifically, I have been studying how to mine Big Text Data across different application domains to find interesting patterns that can provide novel insights to domain experts, which is, otherwise, difficult to perceive due to the scale of the data.&lt;/p&gt;

&lt;h1 id=&quot;research-statement-by-shubhra-kanti-karmaker-santu&quot;&gt;Research Statement by Shubhra Kanti Karmaker (“Santu”)&lt;/h1&gt;
&lt;p&gt;The goal of my research is to develop intelligent systems by exploiting the web-scale Big Data which can enhance human’s perception of the real world and thus, facilitate informed decision making in a wide range of application areas. Broadly, my research interest lies at the intersection of Text Mining, Natural Language Processing and Information Retrieval. More specifically, I have been studying how to mine Big Text Data across different application domains to find interesting patterns that can provide novel insights to domain experts, which is, otherwise, difficult to perceive due to the scale of the data.
	The primary objective of Big Data applications is to help an organization make more informed decisions by analyzing large volumes of data. The power of big data enables a lot of important application areas including intelligent healthcare systems, Automated Quality Control in Manufacturing, Recommender Systems, User Behavior Analysis, Cyber Security and Intelligence, Crime Prediction and Prevention, Acceleration of Scientific Discovery, Stock Market prediction and so on. These intelligent systems help in improving our quality of life and thus, contribute to build a better society.
There are multiple prominent challenges in Big Data domain, including but not limited to: 1) Filtering small amount of relevant data from the ocean of Big Data, 2) Deeper analysis/mining of the relevant data to reveal patterns/insights, and 3) Preserving the privacy of sensitive information of the users. To address these challenges, I have conducted research in the following major directions: 1) Influence Mining in Stream Text Data (related to deeper analysis of relevant data), 2) Retrieval in E-commerce domain (related to filtering relevant data), 3) General techniques for Text Data Mining (again, related to deeper analysis of relevant data) and 4) Privacy Preserving Data Mining (related to privacy of sensitive information).&lt;/p&gt;

&lt;h1 id=&quot;research-accomplishments&quot;&gt;Research Accomplishments&lt;/h1&gt;
&lt;p&gt;I have published 9 (nine) research papers at premier conferences including ACM SIGIR [2], WWW [3], ACM CIKM [1, 4, 5], IEEE CEC [6] and WPES [8]. I also have 2 (two) more research papers currently under submission. As a researcher, I am focused on developing general algorithms that are grounded on solid theoretical foundations and also empirically effective. The benefit of grounding a method on theory is that the behavior of the algorithm can be systematically analyzed, but ultimately, the algorithms/models have to be useful in real-world products, thus, empirical validation of the proposed algorithm is necessary. As a result, my research outcomes generally include creation of new models and algorithms that are immediately applicable to multiple application domains. Following this philosophy, I have developed models/algorithms for analyzing Big Text Data where human and machine can interactively collaborate to complete the desired goal task. My research so far has touched the following directions:&lt;/p&gt;

&lt;h1 id=&quot;influence-mining-in-stream-text-data&quot;&gt;Influence Mining in Stream Text Data&lt;/h1&gt;
&lt;p&gt;This is a new research direction/ area which I have introduced during my PhD. The motivation for this direction is as follows: A crucial component of any intelligent system is to understand and predict the behavior of its users. A correct model of the user behavior enables the system to perform effectively to better serve the users need. While much work has been done on user behavior modeling based on historical activity data, little attention has been paid to how external factors influence the user behavior, which is clearly important for improving an intelligent system. The influence of external factors on user behavior are mostly reflected in two different ways: 1) Through significant growth of users’ thirst about information related to the external factors (e.g., user may conduct a lot of search related to a popular event), and 2) Through the user generated contents that are directly/indirectly related to the external factors (e.g. user may tweet about a particular event). To capture these two aspects of user behavior, I introduced Influence Models for Information Thirst as well as for Content Generation, respectively.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Influence Models for Information Thirst&lt;/strong&gt;: I introduced a new data mining problem, i.e., how to mine the influence of real world events on users’ information thirst, which is important both for social science research and for designing better search engines for users. I solved this mining problem by proposing computational measures that quantify the influence of an event on a query to identify triggered queries and then, proposing a novel extension of Hawkes process to model the evolutionary trend of the influence of an event on search queries. Evaluation results using news articles and search log data show that the proposed approach is effective for identification of queries triggered by events reported in news articles and characterization of the influence trend over time. This work [3] was published in World Wide Web Conference (WWW), 2017. 
However, the problem formulation in [3] was based on the strong assumption that each event poses its influence independently. This assumption is unrealistic as there are many correlated events in the real world which influence each other and thus, would pose a joint influence on the user search behavior rather than posing influence independently. To relax this assumption, I proposed a Joint Influence Model [1] based on the Multivariate Hawkes Process which captures the interdependence among multiple events in terms of their influence. This work has been published at CIKM, 2018.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Influence Models for Content Generation&lt;/strong&gt;: Influence Models for Information Thirst discussed above usually involve mining a single stream of text (e.g. search query log) with respect to an external influential factor (e.g., a popular event). A more challenging problem is to model the influences between two/multiple streams of text. More specifically, I introduced another new problem, where, the task is to model how content generation in one stream influences the generation of contents in the other stream. For example, research papers published by machine learning conferences like NIPS, ICML often have a direct influence on the papers published by more applied conferences like KDD, SIGIR etc. One particular limitation of the existing text generation techniques is that most of them assume the generation process is static, i.e., it does not evolve over time. This clearly limits its application on text stream data as most text stream data often evolve over time showing distinct patterns at different instant of timestamps. The influence from co-related text streams often contribute to these temporal patterns. To capture these influence dynamics, I proposed a deep learning architecture based Dynamic Text Generation Process (DTG) [9]. I demonstrated how DTG can be learnt by mining multiple streams of text and then be applied to two different useful text mining applications including influence inference among different research communities (e.g., between NIPS and SIGKDD) and forecasting of future contents to be generated. This work is currently under submission at KDD, 2019.&lt;/p&gt;

&lt;h1 id=&quot;e-commerce-search&quot;&gt;E-Commerce Search&lt;/h1&gt;
&lt;p&gt;E-Commerce (E-Com) search is an important emerging new application of information retrieval. Virtually all major retailers have their own product search engines, with popular engines processing millions of query requests per day. As E-shopping becomes increasingly popular, optimization of their search quality is increasingly important since an improved E-Com search engine can potentially save all users’ time while increasing their satisfaction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning to Rank for E-Com Search&lt;/strong&gt;: Over the past decade, Learning to Rank (LETOR) methods, which involve applying machine learning techniques on ranking problems, have proven to be very successful in optimizing search engines. However, applications of these methods to any search engine optimization would still face many practical challenges, notably how to define features, how to convert the search log data into effective training sets, how to obtain relevance judgments including both explicit judgments by humans and implicit judgments based on search log, and what objective functions to optimize for specific applications. 
In this research [2], I discussed the practical challenges in applying learning-to-rank methods to E-Com search, including the challenges in feature representation, obtaining reliable relevance judgments, and optimally exploiting multiple user feedback signals such as click rates, add-to-cart ratios, order rates, and revenue. This study yielded multiple interesting insights and results regarding E-Com search practice, that received significant attention from the search industry including Walmart, Flipkart, @Unbxd etc and started a collaboration between me and @Unbxd with a grant money of $40,000. The research findings were published in SIGIR 2017, which also attracted significant attention from the researchgate community with more than 3,000 reads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evaluation for E-Com Search&lt;/strong&gt;: In this research [10], I looked deeper into evaluation metrics for E-Com search. While, nDCG is a commonly used measure for evaluating information retrieval systems, it is usually computed for a fixed cutoff k, i.e., nDCG@k and some fixed discounting coefficient. Such a conventional query-independent way to compute nDCG does not accurately reflect the utility of search results perceived by an individual user for E-Com search, where, the user’s exploration depth varies a lot across different products. To address this limitation, I studied query-specific nDCG to evaluate E-com search engines, particularly to see whether using a query-specific nDCG would lead to a different conclusion about the relative performance of multiple LETOR methods than using the conventional query-independent NDCG would otherwise. Our experimental results show that the relative ranking of LETOR methods using query-specific nDCG can be dramatically different from those using the query-independent nDCG at the individual query level. Although we conducted our evaluation in the context of 
E-Com Search, the findings of this work may potentially improve the current practice of evaluating all kinds of search engines. This work is currently under review at JASIST.&lt;/p&gt;

&lt;h1 id=&quot;general-techniques-for-text-data-mining&quot;&gt;General Techniques for Text Data Mining&lt;/h1&gt;
&lt;p&gt;I have developed multiple general purpose Text Mining techniques which can be applied across a wide variety of application domains. The main focus of this research direction is to make Text Mining accessible to a wide variety of users and enable non-experts to play with different text mining techniques without worrying about underlying technical details. Below, I present three such general techniques:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generative Language Model for Concept Tagging&lt;/strong&gt;: One fundamental challenge in any Text Data Management system is to transform the unstructured text data into a more organized structure by tagging text segments with relevant concepts. Although, there has been numerous works that try to solve this problem through supervised classification, obtaining a lot of labeled data for supervised learning is often infeasible and costly. To address this challenge, I proposed a new approach based on generative feature language models that can mine the implicit concepts more effectively through unsupervised statistical learning [5] without requiring any labeled data from the users. Experiments on a customer product review data showed that my method outperforms existing algorithms by a large margin. I also created and published eight new data sets to facilitate evaluation of this task in English. This research resulted in a publication at ACM CIKM, 2016.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SOFSAT&lt;/strong&gt;: Towards a Set-like Operator based Framework for Semantic Analysis of Text:  As data reported by humans about our world, text data play a very important role in all data mining applications, yet, how to develop a general text analysis system to support all text mining applications is a difficult challenge. To address this challenge, I introduced SOFSAT [11], a new framework that can support set-like operators for semantic analysis of natural text data with variable text representations. It includes three basic set-like operators—TextIntersect, TextUnion, and TextDifference—that are analogous to the corresponding set operators intersection, union, and difference, respectively, which can be applied to any representation of text data, and different representations can be combined via transformation functions that map text to and from any representation. Just as the set operators can be flexibly combined iteratively to construct arbitrary subsets or supersets based on some given sets, I showed that the corresponding text analysis operators can also be combined flexibly to support a wide range of analysis tasks that may require different workflows, thus enabling an application developer to “program” a text mining application by using SOFSAT as an application programming language for text analysis. This position paper has been accepted for publication at SIGKDD Explorations 2018.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Text Based Forecasting System&lt;/strong&gt;: Forecasting of time series [7] is highly beneficial (and necessary) for optimizing decisions, yet a very challenging problem; using only the historical values of the time series is often insufficient. To address this limitation, I collaborated with two first year PhD students to study the problem of how to construct effective additional features based on related text data for time series forecasting [4]. More specifically, we proposed to use contemporary text data (e.g., news articles) for constructing additional features that can help predicting external time series variables like stock price. We evaluated feature effectiveness through a data set for predicting stock price changes using news text and found Text-based features outperform time series-based features for stock prediction. This work was published in CIKM, 2017.&lt;/p&gt;

&lt;h1 id=&quot;privacy-preserving-data-mining&quot;&gt;Privacy Preserving Data Mining&lt;/h1&gt;
&lt;p&gt;The promise of big data relies on the release and aggregation of data sets. When these data sets contain sensitive information about individuals, it has been scalable and convenient to protect the privacy of these individuals by de-identification. However, studies show that the combination of de-identified data sets with other data sets risks re-identification of some records. Yet, there has been no attempt to quantify such re-identification risk due to the hardness of the problem, which is due to potentially unlimited number of factors that have impact over the re-identification risk. In this work [8], I introduced a general probabilistic re-identification framework (NRF) that can be instantiated in specific contexts to estimate the probability of compromises (re-identification risk) based on explicit assumptions. As a case study, I showed how we can apply NRF to analyze and quantify the risk of re-identification arising from releasing de-identified medical data in the context of publicly-available social media data. This research has been published by the renowned workshop WPES 2018, which was held in conjunction with ACM CCS 2018.&lt;/p&gt;

&lt;h1 id=&quot;ongoing-works-and-research-agenda&quot;&gt;Ongoing Works and Research Agenda&lt;/h1&gt;
&lt;p&gt;My long term research goal is to augment human intelligence with machine intelligence by analyzing patterns hidden inside the web-scale BIG Data. I strongly believe that we will achieve far more as a society if human cognition can efficiently collaborate with the immense computational power of the machines. As part of that long-term goal, I have set the following short-term research plan:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A General purpose Text Mining Programming Language&lt;/strong&gt;: Currently, SOFSAT [11] is a hypothetical Text Mining language with no implementation. So, one of my primary research agenda is the realization of SOFSAT system and releasing it as a general toolkit which can be used easily by non-experts (e.g. business professionals, government officials) to quickly analyze a large amount of documents. Once implemented, programmers will be able to include SOFSAT as a package and write efficient and concise text mining code without worrying about the underlying complexity of the mining algorithms. I have provided a detailed roadmap for future research for this new direction in [11], including, Full specification of the SOFSAT text analysis language, implementation of different Set-like operators, incorporating a wide variety of text representations, Optimization of SOFSAT system etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next Generation Conversational AI Systems&lt;/strong&gt;: Conversational AI agents/Chatbots, e.g., Siri, Alexa, Cortana etc. are now very popular, who also serve as intelligent digital assistants. The core of these conversation AI systems is the language understanding task, which consists of user intent identification, entity extraction, action selection and text response generation. There are a lot of open challenges in this domain including how to understand user intent given that the same intent can be expressed in many different ways, how to deal with ambiguous entities present in user utterances, how to tackle complex utterances including multiple intents, how to generate text to provide the correct response to the user, etc. I plan to address these challenges by formulating meaningful representations for user intents, entities and actions, which in turn, conform with the representation of the utterance itself. I also plan to extend the language understanding power of the Conversational AI Systems by plugging in SOFSAT framework as a module inside these agents.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next Generation E-Commerce Search Engine&lt;/strong&gt;: The state-of-the-art of E-commerce search is still in its infancy with a lot of open challenges to be addressed. Some prominent challenges in this domain include 
Dynamic Relevance of Items, Noisy Product Catalog, Expertise transfer learning for domain specific E-com search and Cold Start problem for new products. I believe my previous research experience with E-Commerce Search and Text Mining would provide me a significant edge towards solving these problems.&lt;/p&gt;

&lt;p&gt;My research is highly interdisciplinary and is related to many areas in Computer science and Information Science, particularly Data Mining, Natural Language processing, Information retrieval, Data Security and Privacy, Online Education Platforms and Health informatics. I’m looking forward to collaborating with people in all these areas.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Shubhra Kanti Karmaker Santu, Liangda Li, Yi Chang and Chengxiang Zhai . “JIM: Joint Influence Modeling for Collective Search Behavior “. in ACM CIKM, 2018&lt;/p&gt;

&lt;p&gt;[2] Shubhra Kanti Karmaker Santu, Parikshit Sondhi and ChengXiang Zhai. “On Application of Learning to Rank for E-Commerce Search. In proceedings of ACM SIGIR, 2017: 475-484.&lt;/p&gt;

&lt;p&gt;[3] Shubhra Kanti Karmaker Santu, Liangda Li, Dae Hoon Park, Chengxiang Zhai, Yi Chang, “Modeling the Influence of Popular Trending Events on User Search Behavior”, Proceedings of the 26th International World Wide Web Conference (WWW), 2017: 535-544.&lt;/p&gt;

&lt;p&gt;[4] Yiren Wang, Dominic Seyler, Shubhra Kanti Karmaker Santu and Chengxiang Zhai. “A Study of Feature Construction for Text-based Forecasting of Time Series Variables”.  ACM CIKM, 2017: 2347-2350  [Short Paper].&lt;/p&gt;

&lt;p&gt;[5] Shubhra Kanti Karmaker Santu, Parikshit Sondhi and ChengXiang Zhai. “Generative Feature Language Models for Mining Implicit Features from Customer Reviews”. In proceedings of ACM CIKM, 2016: 929-938.&lt;/p&gt;

&lt;p&gt;[6] Shubhra Kanti Karmaker Santu, Md. Mustafizur Rahman, Md. Monirul Islam and Kazuyuki Murase.  “Towards better generalization in Pittsburgh Learning Classifier Systems”.  In IEEE Congress on Evolutionary Computation (IEEE CEC), 2014  (pp. 1666-1673). IEEE.&lt;/p&gt;

&lt;p&gt;[7] Md. Mustafizur Rahman, Shubhra Kanti Karmaker Santu, Md. Monirul Islam and Kazuyuki Murase.  “Forecasting Time Series - A Layered Ensemble Architecture”.  In International Joint Conference on Neural Networks (IJCNN), 2014 (pp. 210-217). IEEE.&lt;/p&gt;

&lt;p&gt;[8] Shubhra Kanti Karmaker Santu, Vincent Bindschaedler, Chengxiang Zhai and Carl A. Gunter. “NRF: a Naive Probabilistic Re-identification Framework”. in WPES@ACM CCS, 2018.&lt;/p&gt;

&lt;p&gt;[9] Shubhra Kanti Karmaker Santu, Bin Bi, Hao Ma, ChengXiang Zhai and Kuansan Wang. “Dynamic Text Generation Process”. To be submitted to WWW 2019.&lt;/p&gt;

&lt;p&gt;[10] Shubhra Kanti Karmaker Santu, Parikshit Sondhi and ChengXiang Zhai. “Query-Specific Customization of nDCG: A Case-Study with Learning-to-Rank Methods”. Submitted to JASIST.&lt;/p&gt;

&lt;p&gt;[11] Shubhra Kanti Karmaker Santu, Chase Geigle, Duncan Ferguson, William Cope, Mary Kalantzis, Duane Searsmith and Chengxiang Zhai. “SOFSAT: Towards a Set-like Operator based Framework for Semantic Analysis of Text”, accepted for publication at SIGKDD Explorations 2018.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">Broadly, my research interest lies at the intersection of Text Mining, Natural Language Processing and Information Retrieval. More specifically, I have been studying how to mine Big Text Data across different application domains to find interesting patterns that can provide novel insights to domain experts, which is, otherwise, difficult to perceive due to the scale of the data.</summary></entry></feed>