<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-10-02T12:20:55-07:00</updated><id>/feed.xml</id><title type="html">Academic Homepage</title><subtitle>personal description</subtitle><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><entry><title type="html">Multi-Perspective Narrative (MPN) Understanding / Braiding</title><link href="/posts/2023/08/CAMPeN/" rel="alternate" type="text/html" title="Multi-Perspective Narrative (MPN) Understanding / Braiding" /><published>2023-08-24T00:00:00-07:00</published><updated>2023-08-24T00:00:00-07:00</updated><id>/posts/2023/08/Narrative</id><content type="html" xml:base="/posts/2023/08/CAMPeN/">&lt;p&gt;The objective of this research is to design and develop a novel human-AI collaborative framework that can braid the Overlapping, Unique, and Conflicting information from a pair of alternative narratives into a single coherent summary.&lt;/p&gt;

&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/CAMPeN.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:450px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-campen&quot;&gt;Overview of Project CAMPeN&lt;/h1&gt;
&lt;p&gt;Multi-Perspective Narratives (MPNs) are ubiquitous and very useful for verifying information from different alternative narratives, and thus, MPNs facilitate more informed decisions by providing a concise overall picture of the current situation. Despite great progress in the area of natural language processing (NLP), computers are still far from being able to analyze multi-perspective narratives accurately; addressing this limitation is the focus of this project.&lt;/p&gt;

&lt;p&gt;In this ongoing project, we are developing a novel human-AI collaborative framework called CAMPeN (``Collaborative Analytics of Multi-Perspective Narratives’’), where the AI, given multiple alternative narratives as input, first extracts a set of candidate clauses w.r.t. the Overlap-Unique-Conflict criteria, separately, in a zero-shot fashion. Next, the human actively verifies clauses that were labeled with low confidence by the AI. Finally, the machine braids the high-confidence/verified clauses to construct the ultimate Overlap-Unique-Conflict style summary, which will be presented to the user. The major benefits of the proposed framework are two-fold: 1) it enables domain experts in fields other than machine learning/NLP (e.g., a military general) to quickly dig out/verify interesting hypotheses from multiple alternative narratives/descriptions without worrying about the underlying computational techniques and thus, democratizes AI, and 2) it can quickly verify facts and claims about real-world events by analyzing alternative narratives and braid them into a single narrative with a higher degree of Information Assurance.&lt;/p&gt;

&lt;p&gt;This project adopts both zero-shot and reinforcement learning approaches for extracting overlapping, unique, and conflicting information from alternative narratives that can be trained in a self-supervised fashion without requiring a large collection of training data; therefore, the proposed framework needs minimal human supervision in comparison to the existing Multi-Document Summarization techniques. Additionally, the project borrows intuitions and insights from the classical set theory and applies the properties of set operators to develop novel reward/loss functions to enable effective training of reinforcement learning-based extraction networks.&lt;/p&gt;

&lt;h1 id=&quot;objectives-of-project-campen&quot;&gt;Objectives of Project CAMPeN&lt;/h1&gt;
&lt;p&gt;The proposed work includes the following objectives.&lt;/p&gt;

&lt;ol&gt;

&lt;li&gt;Design and develop a novel human-AI collaborative framework that can braid the Overlapping, Unique, and Conflicting information from a pair of alternative narratives into a single coherent summary.&lt;/li&gt;
    
&lt;li&gt;Conduct research on how to extract overlapping information from a pair of alternative narratives and paraphrase the overlap.&lt;/li&gt;
    
&lt;li&gt;Given a pair of alternative narratives as inputs, conduct research on how to extract unique information present exclusively in each input narrative and identify interesting, unique information.&lt;/li&gt;
    
&lt;li&gt;Conduct research on how to extract conflicting information from a pair of alternative narratives and how to resolve the conflict via effective human-AI collaboration.&lt;/li&gt;

&lt;li&gt;Design and Develop novel metrics for quantifying Author Influence by applying the proposed Human-AI collaborative framework.&lt;/li&gt;

&lt;li&gt;Conduct a thorough quantitative and qualitative evaluation of the proposed human-AI collaborative framework.&lt;/li&gt;

&lt;li&gt;Determine whether the proposed Human-AI collaborative framework performs similarly or differently in a language other than English.&lt;/li&gt;

 &lt;/ol&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">The objective of this research is to design and develop a novel human-AI collaborative framework that can braid the Overlapping, Unique, and Conflicting information from a pair of alternative narratives into a single coherent summary.</summary></entry><entry><title type="html">Knowledge Grounding via Meta-Conversation</title><link href="/posts/2023/08/ilab/" rel="alternate" type="text/html" title="Knowledge Grounding via Meta-Conversation" /><published>2023-08-23T00:00:00-07:00</published><updated>2023-08-23T00:00:00-07:00</updated><id>/posts/2023/08/iLab</id><content type="html" xml:base="/posts/2023/08/ilab/">&lt;p&gt;This is an ongoing project in my lab, where we are developing a “Meta-Conversation Framework” to create dialog-based interactive laboratory experiences for middle school science students and teachers in the context of simulation-based science experiments.&lt;/p&gt;

&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/iLab.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:500px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-knowledge-grounding-via-meta-conversation&quot;&gt;Overview of Project “Knowledge Grounding via Meta-Conversation”&lt;/h1&gt;
&lt;p&gt;In open-domain dialog systems, it is often uncertain how the end user would expect a new conversation to be grounded and structured. Therefore, the ideal solution must engage in a pre-conversation with the user about their expectations and preferred knowledge base for grounding purposes before the actual conversation happens. In other words, a “Conversation about Conversation”, i.e., a “Meta-Conversation” should happen with the user beforehand.&lt;/p&gt;

&lt;p&gt;Based on this idea, we are currently developing an Artificial Intelligence-based Conversational Framework to create dialog-based interactive laboratory experiences for middle school science students and teachers in the context of simulation-based science experiments. A key component of the framework is an intelligent conversational agent (SimPal) that actively learns from teachers through a “Meta-Conversation” to solicit their instructional goals associated with simulation experiments and store them using a computational representation. In other words, the school teacher actively teaches the machine/agent what the instructional goals are for a particular scientific experiment in plain natural language. The agent then uses this representation to facilitate and customize an interactive knowledge-grounded conversation (powered by state-of-the-art Large Language Models) with students as they run experiments to enhance their learning experience. Unlike existing intelligent tutoring systems and pedagogical conversational agents, SimPal can work with any off-the-shelf third-party simulations, a unique feature of this project enabled by our proposed Meta-Conversation technique.&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">This is an ongoing project in my lab, where we are developing a “Meta-Conversation Framework” to create dialog-based interactive laboratory experiences for middle school science students and teachers in the context of simulation-based science experiments.</summary></entry><entry><title type="html">Knowledge Grounding via Taxonomy Based Prompting</title><link href="/posts/2023/08/PreSASDiL/" rel="alternate" type="text/html" title="Knowledge Grounding via Taxonomy Based Prompting" /><published>2023-08-21T00:00:00-07:00</published><updated>2023-08-21T00:00:00-07:00</updated><id>/posts/2023/08/PreSASDiL</id><content type="html" xml:base="/posts/2023/08/PreSASDiL/">&lt;p&gt;This is an ongoing project where we develop conversational AI techniques by prompting Large Language Models (LLMs) to build a natural language interface between humans and the AutoML tools (e.g., Scikit-Learn), which, in turn, can facilitate acquiring new predictive skills via self-directed learning. To achieve this, we recently introduced a new prompting taxonomy called TeLER to design diverse prompts that can unleash the full potential of LLMs with proper knowledge grounding.&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-knowledge-grounding-via-taxonomy-based-prompting&quot;&gt;Overview of Project “Knowledge Grounding via Taxonomy Based Prompting”&lt;/h1&gt;
&lt;p&gt;A big challenge in democratizing AI is that no single AI model can be pretrained to be skilled in all possible tasks a user may want to perform. Therefore, being able to learn new skills in an ad hoc fashion is essential. To address this challenge, we are developing a Conversational Data Science solution that is capable of acquiring new predictive skills on the fly through intuitive, natural conversations with the user. In our ACM Computing Surveys 2022 paper, we highlighted the core technical challenges that need to be addressed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Accurately understand and define the goal skill(s) to acquire (as defined by a human)&lt;/li&gt;
    
&lt;li&gt;Formulate a precise and relevant Machine Learning (ML) task.&lt;/li&gt;
    
&lt;li&gt;Curate data sets and assign model hyper-parameters accordingly.&lt;/li&gt;
    
&lt;li&gt;Train AutoML models to learn the skill.&lt;/li&gt;

&lt;li&gt;Apply the skill effectively.&lt;/li&gt;

&lt;/ol&gt;

&lt;p&gt;However, one big hurdle in materializing Conversational Data Science is to ensure a conversation that is grounded in a unique data set that the user may provide from an unseen domain on the fly. To address this challenge, we recently proposed a prompting taxonomy called TeLER to design effective prompts for Large Language Models (LLMs) in order to build a natural language interface between humans and the AutoML tools (e.g., Scikit-Learn), which, in turn, facilitates acquiring new predictive skills via self-directed learning. Our experimental results (available on Arxiv) demonstrate the effectiveness of the proposed TeLER-taxonomy-based prompting technique for knowledge grounding.&lt;/p&gt;

&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/TELER.png&quot; alt=&quot;Image not Loading&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We designed the architecture of our “Conversational Data Science” framework with four dialogue states: Data Visualization, Task Formulation, Prediction Engineering, and Result Summary and Recommendation. Each state marks a unique conversation phase, impacting the overall user-system interaction. Multiple LLM instances, serving as “micro-agents’’, ensure a cohesive conversation flow, granting us granular control over the conversation’s progression. In summary, we designed and developed an end-to-end system that demonstrates the viability of “Conversational Data Science” by evaluating the potency of taxonomy-based prompting of LLMs in solving such an ill-defined complex task.&lt;/p&gt;

&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/VIDS.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:650px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">This is an ongoing project where we develop conversational AI techniques by prompting Large Language Models (LLMs) to build a natural language interface between humans and the AutoML tools (e.g., Scikit-Learn), which, in turn, can facilitate acquiring new predictive skills via self-directed learning. To achieve this, we recently introduced a new prompting taxonomy called TeLER to design diverse prompts that can unleash the full potential of LLMs with proper knowledge grounding.</summary></entry><entry><title type="html">Trust and Influence</title><link href="/posts/2023/08/TI/" rel="alternate" type="text/html" title="Trust and Influence" /><published>2023-08-20T00:00:00-07:00</published><updated>2023-08-20T00:00:00-07:00</updated><id>/posts/2023/08/TI</id><content type="html" xml:base="/posts/2023/08/TI/">&lt;p&gt;This ongoing project aims to develop a novel metric to quantify an author’s influence in the narrative braiding process by comparing the final braided narrative against the individual author’s contribution in terms of the Overlap-Unique-Conflict clauses extracted by the CAMPeN framework.&lt;/p&gt;

&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/Influence1.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:300px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-trust-and-influence&quot;&gt;Overview of Project “Trust and Influence”&lt;/h1&gt;
&lt;p&gt;This project focused on developing a novel metric for quantifying author influence in the context of narrative braiding, which is largely an unexplored research area till now. By definition, a braided narrative is created from the contributions of multiple authors. For this thrust, we also assume that there is a separate entity called the editorial board that polishes and edits the raw contributions of individual authors and is in charge of creating the final braided narrative. Under these assumptions, we consider three scenarios for quantifying author influence: 1) Quantify Author Influence for Single Author Contribution - Single Braided Narrative Scenario, 2) Quantify Author Influence for Multiple Authors - Single Braided Narrative Scenario, and 3) Quantify Author Influence for Multiple Authors - Multiple Braided Narratives Scenario. Another basic idea here is that influential authors become more trustworthy over time and serve as reliable sources in the future.&lt;/p&gt;

&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/Influence2.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:450px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">This ongoing project aims to develop a novel metric to quantify an author’s influence in the narrative braiding process by comparing the final braided narrative against the individual author’s contribution in terms of the Overlap-Unique-Conflict clauses extracted by the CAMPeN framework.</summary></entry><entry><title type="html">Utility Centric Evaluation</title><link href="/posts/2023/08/evaluate/" rel="alternate" type="text/html" title="Utility Centric Evaluation" /><published>2023-08-18T00:00:00-07:00</published><updated>2023-08-18T00:00:00-07:00</updated><id>/posts/2023/08/evaluate</id><content type="html" xml:base="/posts/2023/08/evaluate/">&lt;p&gt;In this project, our main goal is to investigate how to make NLP/IR evaluation metrics more utility-centric and robust.&lt;/p&gt;

&lt;h1 id=&quot;overview-of-project-utility-centric-evaluation&quot;&gt;Overview of Project “Utility Centric Evaluation”&lt;/h1&gt;
&lt;p&gt;Previous studies have shown that popular Natural Language Generation (NLG) and Information Retrieval (IR) and evaluation metrics, e.g., nDCG, ROUGE, MAP, are not robust and often do not correlate with the utility perceived by humans. In this project, our main goal is to investigate how to make NLP/IR evaluation metrics more utility-centric.&lt;/p&gt;

&lt;ul&gt;

&lt;li&gt;&lt;b&gt;Utility-Centric Metrics for Evaluating Text Generation Systems:&lt;/b&gt; In ACL 2022, we proposed a gain/utility-based automated metric called Sem-nCG, which is both rank and semantic aware, for evaluating extractive summarization tasks and showed that Sem-nCG exhibits a higher correlation with human judgments than the popular ROUGE metric. In the same year (EMNLP 2022), we proposed a new sentence-level utility-based evaluation metric, called SEM-${F_1}$ (Semantic $F_1$), for evaluating the performance of the Overlap summary generation task. Experimental results show that the proposed SEM-$F_1$ metric yields a higher correlation with human judgment and inter-rater agreement than the traditional ROUGE metric. Very recently, we proposed TELeR, a general taxonomy for benchmarking complex generation tasks using Large Language Models (LLMs). TELeR enables meaningful comparisons across studies, establishing a common standard for prompt design and LLMs' Utility Evaluation; the taxonomy has received great attention from the industry and practitioners.&lt;/li&gt;&lt;br /&gt;




&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/nCG.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:300px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;br /&gt;
 
&lt;li&gt;&lt;b&gt;Utility-Centric Metrics for Evaluating Ranking Systems:&lt;/b&gt; We recently proposed a novel framework for ranking evaluation with expected-utility normalization, where the expected utility is estimated from a randomized ranking of the corresponding documents present in the evaluation set. Our proposed metric demonstrates an average of 21&amp;percnt; increase in Discriminatory Power and a 28&amp;percnt; increase in consistency. In a similar line of research, I previously performed a detailed evaluation of learning-to-rank methods for both Web search and E-Commerce search by exploiting multiple utility-based signals in addition to click rates, i.e., add-to-cart ratios, order rates, and revenue, results of which were published in CIKM 2022 (Best Poster Nomination) and SIGIR 2017 conference proceedings. The E-Commerce search study yielded multiple interesting insights and results that received significant attention from the search industry, including Walmart, Flipkart, @Unbxd, etc.&lt;/li&gt;

&lt;center&gt;
  &lt;div style=&quot;display: flex; justify-content: center;&quot;&gt;&lt;img src=&quot;/images/LBnorm.png&quot; alt=&quot;Image not Loading&quot; style=&quot;height:300px;&quot; align=&quot;middle&quot; /&gt;&lt;/div&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;br /&gt;

&lt;/ul&gt;</content><author><name>Shubhra Kanti Karmaker (Santu)</name><email>sks0086@auburn.edu</email></author><category term="Research" /><category term="Vision" /><summary type="html">In this project, our main goal is to investigate how to make NLP/IR evaluation metrics more utility-centric and robust.</summary></entry></feed>