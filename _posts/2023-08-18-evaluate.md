---
title: "Project Evaluate"
collection: posts
date: 2023-08-18
permalink: /posts/2023/08/evaluate/
tags:
  - Research
  - Vision
---

In this project, our main goal is to investigate how to make NLP/IR evaluation metrics more utility centric and robust.





Overview of Project "Evaluate"
======
Previous studies have shown that popular Natural Language Generation (NLG) and Information Retrieval (IR) and evaluation metrics, e.g., nDCG, ROUGE, MAP, are not robust and often do not correlate with the utility perceived by humans. In this project, our main goal is to investigate how to make NLP/IR evaluation metrics more utility centric.


<ul>

<li><b>Natural Language Generation (NLG) Utility Evaluation:</b> In ACL 2022, we proposed a gain-based automated metric called Sem-nCG, which is both rank and semantic aware, for evaluating extractive summarization tasks and showed that Sem-nCG exhibits a higher correlation with human judgments than the popular ROUGE metric. In the same year (EMNLP 2022), we proposed a new precision-recall style evaluation metric, called SEM-${F_1}$ (Semantic $F_1$), for evaluating the performance of the Overlap summary generation task. Experimental results show that the proposed SEM-$F_1$ metric yields a higher correlation with human judgment and inter-rater-agreement than the traditional ROUGE metric. Very recently, I proposed TELeR, a general taxonomy for benchmarking complex generation tasks using large language models (LLMs). TELeR enables meaningful comparisons across studies, establishing a common standard for prompt design and LLM evaluation; the taxonomy has received high attention from the industry and practitioners.</li><br>


<center>
  <div style='display: flex; justify-content: center;'><img src='/images/nCG.png' alt='Image not Loading' style='height:300px;' align='middle'></div><br>
</center>
<br>
 
<li><b>Ranking Utility Evaluation:</b> We recently proposed a novel framework for ranking evaluation with both upper and expected value normalization, where the expected value is estimated from a randomized ranking of the corresponding documents present in the evaluation set. Our proposed metric demonstrates an average of 21&percnt; increase in Discriminatory Power and a 28&percnt; increase in consistency. In a similar line of research, I previously performed a detailed evaluation of learning-to-rank methods for both Web search and E-Commerce search by exploiting multiple user feedback signals such as click rates, add-to-cart ratios, order rates, and revenue. The E-Commerce search study yielded multiple interesting insights and results that received significant attention from the search industry, including Walmart, Flipkart, @Unbxd, etc.</li>

<center>
  <div style='display: flex; justify-content: center;'><img src='/images/LBnorm.png' alt='Image not Loading' style='height:300px;' align='middle'></div><br>
</center>
<br>

</ul>